{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b8a920-5937-4dca-8399-f830d20b01b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MVP Engenharia de Dados - Notebook 2: Camada SILVER\n",
    "\n",
    "**Objetivo:** Ler os dados brutos da camada Bronze, aplicar regras de qualidade e transformação (T) e preparar os dados para a modelagem.\n",
    "\n",
    "**Processo de Transformação (T):**\n",
    "\n",
    "Este notebook resolve os seguintes problemas de qualidade de dados:\n",
    "\n",
    "1. **Datas e Horas:**\n",
    "    * **Problema:** A coluna `DATA_HORA_GERACAO` foi lida como `string`.\n",
    "    * **Solução:** Convertemos para o tipo `timestamp` usando a função `to_timestamp()`.\n",
    "\n",
    "2. **Valores Monetários:**\n",
    "    * **Problema:** Colunas como `PRECO_VENDA_UNIDADE` e `VR_DESCONTO` foram lidas como `string`, pois continham vírgulas (ex: \"10,90\").\n",
    "    * **Solução:** Usamos `regexp_replace()` para trocar \",\" por \".\" e depois `cast(DecimalType)` para converter em um tipo numérico preciso.\n",
    "\n",
    "3. **Dados de PDV e Sócio (Parsing):**\n",
    "    * **Problema:** A coluna `INFORMACAO_TRIBUTO` continha informações relevantes e outras nem tanto em um único texto (ex: \"POS : 234 ; PDV: Bar Misto;; ALERJ...\").\n",
    "    * **Solução:** Usamos `regexp_extract()` para \"quebrar\" esta coluna em duas novas colunas: `ID_SOCIO` e `NOME_PDV` contendo somente as informações relevantes para a análise.\n",
    "\n",
    "4. **Categorização de Produtos (Nova Dimensão):**\n",
    "    * **Problema:** A tabela de vendas não possui a informação de \"Grupo\" (Categoria) do produto, o que impede a distinção entre comidas, bebidas... na análise final.\n",
    "    * **Solução:** Tratamos a tabela auxiliar `bronze_materiais_raw`, selecionando apenas as colunas de vínculo (`Código` e `Grupo`) e renomeando para `COD_PRODUTO` e `NOME_GRUPO`, criando a tabela `silver_dim_grupos` pronta para o cruzamento.\n",
    "\n",
    "**Carga (Modelagem):**\n",
    "\n",
    "Ao final, separei os dados limpos em três tabelas para preparar a junção na camada Gold:\n",
    "\n",
    "* `default.silver_dim_produto` (Dados do Produto vindos da venda)\n",
    "* `default.silver_fato_vendas` (Dados da Transação)\n",
    "* `default.silver_dim_grupos` (Dados de Categoria vindos do cadastro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c387f68-5f1d-4e40-82b9-c9263756036f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ler a tabela BRONZE\n",
    "bronze_df = spark.read.table(\"default.bronze_vendas_raw\")\n",
    "\n",
    "# 2. Mostrar o Schema\n",
    "bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e980840-1288-4450-a7ca-ccea92d0fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, regexp_replace, regexp_extract, trim\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# Definições para Vendas\n",
    "COLUNA_BAGUNCADA = \"INFORMACAO_TRIBUTO\"\n",
    "timestamp_cols = [\"DATA_HORA_GERACAO\"]\n",
    "money_cols = [\"PRECO_VENDA_UNIDADE\", \"VR_DESCONTO\", \"BC\"]\n",
    "\n",
    "# Carregar as tabelas BRONZE\n",
    "bronze_vendas_df = spark.read.table(\"default.bronze_vendas_raw\")\n",
    "bronze_grupos_df = spark.read.table(\"default.bronze_materiais_raw\")\n",
    "\n",
    "print(\"Processando Vendas...\")\n",
    "\n",
    "# Filtrar somente as notas fiscais emitidas\n",
    "cleaned_df = bronze_vendas_df.filter(col(\"ESTADO\") == \"E\")\n",
    "\n",
    "# Limpar e converter colunas de Data/Hora\n",
    "for col_name in timestamp_cols:\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        col_name, to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    "    )\n",
    "\n",
    "# Limpar e converter colunas de Dinheiro\n",
    "for col_name in money_cols:\n",
    "    cleaned_df = (\n",
    "        cleaned_df.withColumn(\n",
    "            col_name + \"_limpo\", regexp_replace(col(col_name), \",\", \".\")\n",
    "        )\n",
    "        .withColumn(col_name, col(col_name + \"_limpo\").cast(DecimalType(10, 2)))\n",
    "        .drop(col_name + \"_limpo\")\n",
    "    )\n",
    "\n",
    "# Parsing e Extração (Sócio e PDV)\n",
    "cleaned_df = cleaned_df.withColumn(\n",
    "    \"ID_SOCIO\", regexp_extract(col(COLUNA_BAGUNCADA), r\"POS : (\\d+)\", 1)\n",
    ").withColumn(\"NOME_PDV\", regexp_extract(col(COLUNA_BAGUNCADA), r\"PDV: (.*?);\", 1))\n",
    "\n",
    "# Renomear colunas-chave\n",
    "cleaned_df = (\n",
    "    cleaned_df.withColumnRenamed(\"DESCR\", \"NOME_PRODUTO\")\n",
    "    .withColumnRenamed(\"QTD\", \"QUANTIDADE\")\n",
    "    .withColumnRenamed(\"PRECO_VENDA_UNIDADE\", \"PRECO_UNITARIO\")\n",
    "    .withColumnRenamed(\"VR_DESCONTO\", \"VALOR_DESCONTO\")\n",
    "    .withColumnRenamed(\"DATA_HORA_GERACAO\", \"DATA_HORA\")\n",
    "    .withColumnRenamed(\"BC\", \"VALOR_TOTAL_ITEM\")\n",
    ")\n",
    "\n",
    "# Separar: Criar a Dimensão Produto e a Fato Vendas\n",
    "dim_produto_df = cleaned_df.select(\"COD_PRODUTO\", \"NOME_PRODUTO\").distinct()\n",
    "\n",
    "colunas_fato = [\n",
    "    \"NUM_NFCE\", \"NUM_ITEM\", \"DATA_HORA\", \"COD_PRODUTO\",\n",
    "    \"QUANTIDADE\", \"PRECO_UNITARIO\", \"VALOR_TOTAL_ITEM\",\n",
    "    \"VALOR_DESCONTO\", \"ID_SOCIO\", \"NOME_PDV\"\n",
    "]\n",
    "fato_vendas_df = cleaned_df.select(*colunas_fato)\n",
    "\n",
    "\n",
    "# Definições para Grupos\n",
    "print(\"Processando Grupos...\")\n",
    "\n",
    "# Selecionar e renomear colunas para o padrão Silver\n",
    "# Código -> COD_PRODUTO (Para fazer o join depois)\n",
    "# Grupo  -> NOME_GRUPO\n",
    "dim_grupos_df = bronze_grupos_df.select(\n",
    "    col(\"Código\").alias(\"COD_PRODUTO\"),\n",
    "    trim(col(\"Grupo\")).alias(\"NOME_GRUPO\")\n",
    ").distinct()\n",
    "\n",
    "\n",
    "# SALVAMENTO\n",
    "print(\"Salvando tabelas na Camada SILVER\")\n",
    "\n",
    "# Salvar Fato Vendas\n",
    "fato_vendas_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.silver_fato_vendas\")\n",
    "print(\"- silver_fato_vendas: OK\")\n",
    "\n",
    "# Salvar Dim Produto\n",
    "dim_produto_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.silver_dim_produto\")\n",
    "print(\"- silver_dim_produto: OK\")\n",
    "\n",
    "# Salvar Dim Grupos (A Nova!)\n",
    "dim_grupos_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.silver_dim_grupos\")\n",
    "print(\"- silver_dim_grupos: OK\")\n",
    "\n",
    "print(\"\\nSUCESSO! Todas as 3 tabelas Silver foram atualizadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d12cc3-1d6f-4d2b-8a35-4a76301d5e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificação de Qualidade (QA) da Camada Silver\n",
    "\n",
    "Nesta seção evidenciei que todas as regras de negócio e transformações foram aplicadas com sucesso no pipeline.\n",
    "\n",
    "Irei inspecionar o Schema (a estrutura) e os Dados (a amostra) das três tabelas que acabei de criar: `silver_fato_vendas`, `silver_dim_produto` e `silver_dim_grupos`.\n",
    "\n",
    "**Pontos de verificação a observar nas tabelas abaixo:**\n",
    "\n",
    "1. **Tipos de Dados Corrigidos:**\n",
    "    * `DATA_HORA` agora é do tipo `timestamp` (não mais `string`).\n",
    "    * `PRECO_UNITARIO` e `VALOR_TOTAL_ITEM` agora são `decimal` (não mais `string` com vírgula).\n",
    "\n",
    "2. **Regras de Negócio Aplicadas:**\n",
    "    * **Parsing (Extração):** As colunas `ID_SOCIO` e `NOME_PDV` foram extraídas com sucesso da fonte original.\n",
    "    * **Filtro:** Apenas notas com `ESTADO` = 'E' foram processadas (embora isso não seja visível na amostra, o count de linhas seria menor que o da Bronze).\n",
    "    * **Enriquecimento (Categorização):** A nova tabela `silver_dim_grupos` foi gerada e normalizada para permitir a classificação correta dos produtos (comida, bebida...) na etapa Gold.\n",
    "\n",
    "3. **Modelo Otimizado (Colunas Removidas):**\n",
    "    * A `silver_fato_vendas` contém apenas as colunas relevantes para a análise (removendo `ICMS`, `IDORIGEM`, etc.).\n",
    "    * A `silver_dim_produto` contém apenas `COD_PRODUTO` e `NOME_PRODUTO`.\n",
    "    * A `silver_dim_grupos` contém apenas `COD_PRODUTO` e `NOME_GRUPO` (removendo colunas desnecessárias do cadastro original).\n",
    "\n",
    "4. **Verificação de Nulos (Integridade):**\n",
    "    * Foi validado (através de `display()` ou `count()`) que as colunas métricas essenciais para a análise (como `QUANTIDADE`, `PRECO_UNITARIO`, `DATA_HORA`) não contêm valores nulos. Isso garante a integridade dos cálculos de faturamento  e agrupamentos na camada de Análise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5700e620-6371-4faa-8bc9-3ddbed574d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificação pós-silver\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "print(\"Carregando tabelas Silver para verificação\")\n",
    "fato_df = spark.read.table(\"default.silver_fato_vendas\")\n",
    "dim_produto_df = spark.read.table(\"default.silver_dim_produto\")\n",
    "dim_grupos_df = spark.read.table(\"default.silver_dim_grupos\")\n",
    "\n",
    "# Verificação de Nulos\n",
    "print(\"=\"*50)\n",
    "print(\"1. VERIFICANDO INTEGRIDADE DA FATO (VENDAS)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "colunas_criticas = [\"NUM_NFCE\", \"DATA_HORA\", \"COD_PRODUTO\", \"VALOR_TOTAL_ITEM\"]\n",
    "null_counts = fato_df.select([count(when(col(c).isNull(), c)).alias(c) for c in colunas_criticas]).first().asDict()\n",
    "\n",
    "print(\"Nulos encontrados:\")\n",
    "for c, v in null_counts.items():\n",
    "    print(f\"- {c}: {v}\")\n",
    "\n",
    "# Análise de Parsing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. VERIFICANDO PARSING (SÓCIO/PDV)\")\n",
    "print(\"=\"*50)\n",
    "parsing_check = fato_df.select(\n",
    "    count(\"*\").alias(\"Total_Linhas\"),\n",
    "    count(when(col(\"ID_SOCIO\").isNull(), 1)).alias(\"Sem_Socio\"),\n",
    "    count(when(col(\"NOME_PDV\").isNull(), 1)).alias(\"Sem_PDV\")\n",
    ")\n",
    "display(parsing_check)\n",
    "\n",
    "# Verificação de Schemas\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. VERIFICANDO SCHEMAS (ESTRUTURA)\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFATO VENDAS\")\n",
    "fato_df.printSchema()\n",
    "print(\"\\nDIM PRODUTO\")\n",
    "dim_produto_df.printSchema()\n",
    "\n",
    "# Verificação da Tabela de Grupos\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. VERIFICANDO NOVA DIMENSÃO: GRUPOS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nSchema (Deve ter COD_PRODUTO e NOME_GRUPO)\")\n",
    "dim_grupos_df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal de produtos categorizados: {dim_grupos_df.count()}\")\n",
    "\n",
    "print(\"\\nAmostra dos Grupos\")\n",
    "display(dim_grupos_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8825afa-4d97-481f-b83a-4c1f9b7399d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de todas as tabelas\n",
    "tabelas = [\n",
    "    \"default.bronze_vendas_raw\",\n",
    "    \"default.bronze_materiais_raw\",\n",
    "    \"default.silver_fato_vendas\",\n",
    "    \"default.silver_dim_produto\",\n",
    "    \"default.silver_dim_grupos\"\n",
    "]\n",
    "\n",
    "# Mostra todas as tabelas\n",
    "for t in tabelas:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"AMOSTRA DA TABELA: {t}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    display(spark.read.table(t).limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2: Camada SILVER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
